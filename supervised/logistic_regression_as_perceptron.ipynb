{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression as a perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Cost function](#Cost-function)\n",
    "* [Implementation](#Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Linear Algebra recap\n",
    "To multiply two matrices, the number of columns of the first matrix must equal the number of rows of the second matrix.\n",
    "\n",
    "#### Matrix x Vector multiplication (3x2)*(2x1):\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}  a & b \\newline   c & d \\newline   e & f \\end{bmatrix} *\\begin{bmatrix}  x \\newline   y \\newline  \\end{bmatrix} =\\begin{bmatrix}  a*x + b*y \\newline   c*x + d*y \\newline   e*x + f*y\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "#### Matrix x Matrix multiplication, dot product (3x2)*(2x2):\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}  a & b \\newline   c & d \\newline   e & f \\end{bmatrix} *\\begin{bmatrix}  w & x \\newline   y & z \\newline  \\end{bmatrix} =\\begin{bmatrix}  a*w + b*y & a*x + b*z \\newline   c*w + d*y & c*x + d*z \\newline   e*w + f*y & e*x + f*z\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "#### In deep learning:\n",
    "<img src=\"img/lalgebra_recap.png\" width=700 height=500></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression as a perceptron\n",
    "\n",
    "<img src=\"img/perceptron.png\" width=600 height=500></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes. It is also actually a very simple Neural Network of one perceptron. Let's take a look a the mathematical expression of the algorithm.\n",
    "\n",
    "\\begin{equation*}\n",
    "z^{(i)} = W^T x^{(i)} + b\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\n",
    "\\end{equation*}\n",
    "Sigmoid activation: \\begin{equation*}sigmoid(z) = \\frac{1} {1 + e^{-z}}\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity: O(n * epoch) = O(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression has to run on all examples of the training set at least once, so the cost is O(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the MSE as with Linear Regression, we will use Cross-entropy, also called **log-loss** as a way to measure how well our model is predicting the class. Log-loss measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. The formula is this:\n",
    "\n",
    "\\begin{equation*}\n",
    "L(a^{(i)}, y^{(i)}) = -y^{(i)}log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})\n",
    "\\end{equation*}\n",
    "\n",
    "And then for all training samples the cost is computed as:\n",
    "\\begin{equation*}\n",
    "J = \\frac{1}{m} \\sum_{i=1}^{m} L(a^{(i)}, y^{(i)})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps are going to be pretty similar to linear regression:\n",
    "* Initialize parameters\n",
    "* Learn parameters by minimizing the cost by looping:\n",
    "  * Calculate current loss (forward propagation)\n",
    "  * Calculate current gradient (backward propagation)\n",
    "  * Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris_data = iris.data\n",
    "iris_labels = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression only works for two classes {0, 1}**. If we have more, our cost will go downhill the negative spectrum. So in order to get any strange behavior, we select only the ones with 0 and 1 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 0 0 0 0]...\n",
      "Data: [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]...\n",
      "Shape of labels: 100\n",
      "Shape of data: 100\n"
     ]
    }
   ],
   "source": [
    "iris_data = iris_data[iris_labels < 2]\n",
    "iris_labels = iris_labels[iris_labels < 2]\n",
    "print(\"Labels: {}...\".format(iris_labels[:5]))\n",
    "print(\"Data: {}...\".format(iris_data[:5]))\n",
    "print(\"Shape of labels: {}\".format(len(iris_labels)))\n",
    "print(\"Shape of data: {}\".format(iris_labels.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to map predicted values to probabilities, we use the sigmoid function. The function **maps any real value into another value between 0 and 1**. In machine learning, we use sigmoid to map predictions to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n",
      "sigmoid(9.2) = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print(\"sigmoid(9.2) = \" + str(sigmoid(10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the parameters `W` and `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_variables):\n",
    "    w = np.zeros((n_variables, 1), dtype = int)\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = 0.0\n",
      "w = [[0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "n_variables = 3\n",
    "w, b = initialize_parameters(n_variables)\n",
    "print (\"b = \" + str(b))\n",
    "print (\"w = \" + str(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "In forward propagation we pass the information forward on the neural to obtain the predictions in `A`. Simply give the input to the perceptron `X` and the parameters `W`, `b` to obtain the activation output `A`. We also compute the cost to check how our model is performing. Our initial `W` and `b` will be set to zero (don't do it in NN or you won't break simmetry of output), but we will update them each iteration in the backpropagation step.\n",
    "\n",
    "Compute:\n",
    "* $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "* $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(w, b, X, Y):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # compute activation\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    # compute cost\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))\n",
    "    \n",
    "    return A, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "In backward propagation we pass the information backward, specially the gradients. We compute the gradients with the partial derivates of the cost with respect to each parameter. This is so we can update our parameters, also called the weights.\n",
    "\n",
    "<img src=\"img/backprop.png\" width = 600, height = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivatives of the cost with respect to `w` and `b` are as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(w, b, A, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(w, b, dw, db, learning_rate):\n",
    "    w = w - (learning_rate * dw)\n",
    "    b = b - (learning_rate * db)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our helper functions running, we can implement our optimization loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 100\n",
      "train_set_x shape: (100, 4)\n",
      "train_set_y shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "m_train_x = iris_data\n",
    "m_train_y = iris_labels\n",
    "print(\"Number of training examples: m_train = \" + str(len(m_train_x)))\n",
    "print(\"train_set_x shape: \" + str(m_train_x.shape))\n",
    "print(\"train_set_y shape: \" + str(m_train_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have to transform the dataset to be a 4 x 150 matrix, and we got the transposed one. Let's fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (4, 100)\n",
      "weights: (1, 4)\n"
     ]
    }
   ],
   "source": [
    "m_train_x = iris_data.T\n",
    "w, b = initialize_parameters(m_train_x.shape[0])\n",
    "print(\"train_set_x shape: \" + str(m_train_x.shape))\n",
    "print(\"weights: {}\".format(w.T.shape)) # we also do transpose inside the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y, num_iterations, learning_rate = 0.0002, verbose = 0):\n",
    "    \n",
    "    w, b = initialize_parameters(X.shape[0])\n",
    "    \n",
    "    w_epoch = [w]\n",
    "    b_epoch = [b]\n",
    "    cost_epoch = []\n",
    "        \n",
    "    for i in range(num_iterations):\n",
    "        A, cost = forward_prop(w, b, X, Y)\n",
    "        dw, db = backward_prop(w, b, A, X, Y)\n",
    "        w, b = update_parameters(w, b, dw, db, learning_rate)\n",
    "        \n",
    "        # save results\n",
    "        w_epoch.append(w)\n",
    "        b_epoch.append(b)\n",
    "        cost_epoch.append(cost)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(\"Epoch: {} Cost: {}\".format(i, cost))\n",
    "            if verbose > 1:\n",
    "                print(\"- X: {}\".format(X[:5]))\n",
    "                print(\"- Weights: {}\".format(w))\n",
    "                print(\"- Predictions: {}\".format(predictions[:5]))\n",
    "                print(\"- dW: {}\".format(dw[:5]))\n",
    "                print(\"- db: {}\".format(db[:5]))\n",
    "        \n",
    "    return cost, w, b, dw, db, cost_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost, w, b, dw, db, cost_epoch = logistic_regression(m_train_x, \n",
    "                                                     m_train_y, \n",
    "                                                     learning_rate = 0.002, \n",
    "                                                     num_iterations = 1500, \n",
    "                                                     verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bfc3b6e320>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # compute predictions with optimized parameters\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    # convert probabilities to actual predictions (0, 1)\n",
    "    y_hat = np.around(A, decimals=0)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(w, b, m_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 99.0 %\n"
     ]
    }
   ],
   "source": [
    "# Print train/test Errors\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(predictions - m_train_y)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "- Always check the matrix dimmensions. Consider that computing of predictions, according to the formula, `W` is transposed too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links:\n",
    "* None yet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
