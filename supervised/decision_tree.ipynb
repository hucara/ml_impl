{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Cost function](#Cost-function:-Gini-index)\n",
    "* [Implementation](#Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dectree.jpeg\" width=640>\n",
    "\n",
    "Decision Tree algorithms that can be used for classification or regression predictive modeling problems. It is actually of the shape of a binary tree. A node represents a single input variable (X) and a split point on that variable, assuming the variable is numeric. The leaf nodes (also called terminal nodes) of the tree contain an output variable (y) which is used to make a prediction.\n",
    "\n",
    "Once created, a tree can be navigated with a new row of data following each branch with the splits until a final prediction is made.\n",
    "\n",
    "Creating a binary decision tree is actually a process of dividing up the input space. A greedy approach is used to divide the space called recursive binary splitting. This is a numerical procedure where all the values are lined up and different split points are tried and tested using a cost function.\n",
    "\n",
    "The split with the best cost (lowest cost because we minimize cost) is selected. All input variables and all possible split points are evaluated and chosen in a greedy manner based on the cost function. Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity: O(n * epoch) = O(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression has to run on all examples of the training set at least once, so the cost is O(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function: Gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a decision tree we will use the Gini index as the cost function. This will tell us how good our splits are, considering a split as a division in the training sample of one input atribute at one specific value.\n",
    "\n",
    "* **Regression**: The cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rectangle.\n",
    "* **Classification**: The Gini cost function is used which provides an indication of how pure the nodes are, where node purity refers to how mixed the training data assigned to each node is.\n",
    "\n",
    "To compute the split, the Gini index will behave as a greedy algorithm looking of how mixed are the output classes in the two groups created by the split. When the value of this cost is 1.0, means the split was a perfect separation. The worst split will be 0.5, meaning it leaves 50/50 classes in each group result. \n",
    "\n",
    "The steps for computing this are:\n",
    "* Calculate the proportion of classes in each group.\n",
    "* Obtain gini index for each group, weighting by size of group with respect of all samples in the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2, 0. ],\n",
       "       [4.9, 3. , 1.4, 0.2, 0. ],\n",
       "       [4.7, 3.2, 1.3, 0.2, 0. ],\n",
       "       [4.6, 3.1, 1.5, 0.2, 0. ],\n",
       "       [5. , 3.6, 1.4, 0.2, 0. ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# iris preprocessing. Concatenate y column in dataset\n",
    "iris = datasets.load_iris()\n",
    "iris_data = np.insert(iris.data, 4, iris.target, axis=1)\n",
    "iris_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting a dataset involves iterating over each row, checking if the attribute value is below or above the split value and assigning it to the left or right group respectively. Once we have the two groups, we can then use our Gini score above to evaluate the cost of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(index, cut_value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < cut_value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "            \n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2, 0. ],\n",
       "       [4.9, 3. , 1.4, 0.2, 0. ],\n",
       "       [4.7, 3.2, 1.3, 0.2, 0. ],\n",
       "       [4.6, 3.1, 1.5, 0.2, 0. ],\n",
       "       [5. , 3.6, 1.4, 0.2, 0. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left group: \n",
      "[array([4.9, 3. , 1.4, 0.2, 0. ]), array([4.6, 3.1, 1.5, 0.2, 0. ]), array([4.4, 2.9, 1.4, 0.2, 0. ]), array([4.9, 3.1, 1.5, 0.1, 0. ]), array([4.8, 3. , 1.4, 0.1, 0. ]), array([4.3, 3. , 1.1, 0.1, 0. ]), array([5. , 3. , 1.6, 0.2, 0. ]), array([4.8, 3.1, 1.6, 0.2, 0. ]), array([4.9, 3.1, 1.5, 0.1, 0. ]), array([4.9, 3.1, 1.5, 0.1, 0. ]), array([4.4, 3. , 1.3, 0.2, 0. ]), array([4.5, 2.3, 1.3, 0.3, 0. ]), array([4.8, 3. , 1.4, 0.3, 0. ]), array([6.9, 3.1, 4.9, 1.5, 1. ]), array([5.5, 2.3, 4. , 1.3, 1. ]), array([6.5, 2.8, 4.6, 1.5, 1. ]), array([5.7, 2.8, 4.5, 1.3, 1. ]), array([4.9, 2.4, 3.3, 1. , 1. ]), array([6.6, 2.9, 4.6, 1.3, 1. ]), array([5.2, 2.7, 3.9, 1.4, 1. ]), array([5. , 2. , 3.5, 1. , 1. ]), array([5.9, 3. , 4.2, 1.5, 1. ]), array([6. , 2.2, 4. , 1. , 1. ]), array([6.1, 2.9, 4.7, 1.4, 1. ]), array([5.6, 2.9, 3.6, 1.3, 1. ]), array([6.7, 3.1, 4.4, 1.4, 1. ]), array([5.6, 3. , 4.5, 1.5, 1. ]), array([5.8, 2.7, 4.1, 1. , 1. ]), array([6.2, 2.2, 4.5, 1.5, 1. ]), array([5.6, 2.5, 3.9, 1.1, 1. ]), array([6.1, 2.8, 4. , 1.3, 1. ]), array([6.3, 2.5, 4.9, 1.5, 1. ]), array([6.1, 2.8, 4.7, 1.2, 1. ]), array([6.4, 2.9, 4.3, 1.3, 1. ]), array([6.6, 3. , 4.4, 1.4, 1. ]), array([6.8, 2.8, 4.8, 1.4, 1. ]), array([6.7, 3. , 5. , 1.7, 1. ]), array([6. , 2.9, 4.5, 1.5, 1. ]), array([5.7, 2.6, 3.5, 1. , 1. ]), array([5.5, 2.4, 3.8, 1.1, 1. ]), array([5.5, 2.4, 3.7, 1. , 1. ]), array([5.8, 2.7, 3.9, 1.2, 1. ]), array([6. , 2.7, 5.1, 1.6, 1. ]), array([5.4, 3. , 4.5, 1.5, 1. ]), array([6.7, 3.1, 4.7, 1.5, 1. ]), array([6.3, 2.3, 4.4, 1.3, 1. ]), array([5.6, 3. , 4.1, 1.3, 1. ]), array([5.5, 2.5, 4. , 1.3, 1. ])]\n",
      "Right group: \n",
      "[array([5.1, 3.5, 1.4, 0.2, 0. ]), array([4.7, 3.2, 1.3, 0.2, 0. ]), array([5. , 3.6, 1.4, 0.2, 0. ]), array([5.4, 3.9, 1.7, 0.4, 0. ]), array([4.6, 3.4, 1.4, 0.3, 0. ]), array([5. , 3.4, 1.5, 0.2, 0. ]), array([5.4, 3.7, 1.5, 0.2, 0. ]), array([4.8, 3.4, 1.6, 0.2, 0. ]), array([5.8, 4. , 1.2, 0.2, 0. ]), array([5.7, 4.4, 1.5, 0.4, 0. ]), array([5.4, 3.9, 1.3, 0.4, 0. ]), array([5.1, 3.5, 1.4, 0.3, 0. ]), array([5.7, 3.8, 1.7, 0.3, 0. ]), array([5.1, 3.8, 1.5, 0.3, 0. ]), array([5.4, 3.4, 1.7, 0.2, 0. ]), array([5.1, 3.7, 1.5, 0.4, 0. ]), array([4.6, 3.6, 1. , 0.2, 0. ]), array([5.1, 3.3, 1.7, 0.5, 0. ]), array([4.8, 3.4, 1.9, 0.2, 0. ]), array([5. , 3.4, 1.6, 0.4, 0. ]), array([5.2, 3.5, 1.5, 0.2, 0. ]), array([5.2, 3.4, 1.4, 0.2, 0. ]), array([4.7, 3.2, 1.6, 0.2, 0. ]), array([5.4, 3.4, 1.5, 0.4, 0. ]), array([5.2, 4.1, 1.5, 0.1, 0. ]), array([5.5, 4.2, 1.4, 0.2, 0. ]), array([5. , 3.2, 1.2, 0.2, 0. ]), array([5.5, 3.5, 1.3, 0.2, 0. ]), array([5.1, 3.4, 1.5, 0.2, 0. ]), array([5. , 3.5, 1.3, 0.3, 0. ]), array([4.4, 3.2, 1.3, 0.2, 0. ]), array([5. , 3.5, 1.6, 0.6, 0. ]), array([5.1, 3.8, 1.9, 0.4, 0. ]), array([5.1, 3.8, 1.6, 0.2, 0. ]), array([4.6, 3.2, 1.4, 0.2, 0. ]), array([5.3, 3.7, 1.5, 0.2, 0. ]), array([5. , 3.3, 1.4, 0.2, 0. ]), array([7. , 3.2, 4.7, 1.4, 1. ]), array([6.4, 3.2, 4.5, 1.5, 1. ]), array([6.3, 3.3, 4.7, 1.6, 1. ]), array([5.9, 3.2, 4.8, 1.8, 1. ]), array([6. , 3.4, 4.5, 1.6, 1. ])]\n"
     ]
    }
   ],
   "source": [
    "left, right = make_split(1, 3.2, iris_data[:90])\n",
    "print(\"Left group: \")\n",
    "print(left)\n",
    "print(\"Right group: \")\n",
    "print(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(groups, classes, y_col_idx = -1):\n",
    "    \n",
    "    # count samples at a split point\n",
    "    n_instances = np.sum([len(group) for group in groups])\n",
    "    \n",
    "    # obtain and sum gini indexes for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        group_size = len(group)\n",
    "        \n",
    "        if group_size == 0:\n",
    "            continue\n",
    "        \n",
    "        # for that group, count examples of each different lasses and evaluate according to group size\n",
    "        group_score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[y_col_idx] for row in group].count(class_val) / group_size\n",
    "            group_score += p * p\n",
    "            \n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - group_score) * (group_size / n_instances)\n",
    "        \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.0\n",
      "0.30853174603174616\n"
     ]
    }
   ],
   "source": [
    "# test Gini values\n",
    "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1], y_col_idx = -1))\n",
    "print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1], y_col_idx = -1))\n",
    "print(gini_index([left, right], [0, 1, 2], y_col_idx = -1))\n",
    "#print(gini_index(right, [0, 1, 2], y_col_idx = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset, we must check every value on each attribute as a candidate split, evaluate the cost of the split and find the best possible split we could make. Once the best split is found, we can use it as a node in our decision tree.\n",
    "\n",
    "This is an exhaustive and greedy algorithm that resumes in:\n",
    "* Splitting a dataset.\n",
    "* Evaluating all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(dataset, y_col_idx, verbose = 0):\n",
    "    class_values = list(set(row[y_col_idx] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    num_variables = len(dataset[0]) - 1\n",
    "    \n",
    "    for index in range(num_variables):\n",
    "        if verbose > 0: print(\"\\nSelecting split value for variable {}:\".format(index))\n",
    "        for row in dataset:\n",
    "            groups = make_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values, y_col_idx)\n",
    "            #print('  Var{} < {:.2f} Gini={:.3f}'.format(index, row[index], gini))\n",
    "            \n",
    "            # if best split, keep it \n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "                \n",
    "        if verbose > 0: print('  Cut: < {:.2f} Gini={:.3f}'.format(b_value, b_score))\n",
    "                \n",
    "    return {'variable idx':b_index, 'cut value':b_value, 'groups':b_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting split value for variable 0:\n",
      "  Cut: < 5.60 Gini=0.493\n",
      "\n",
      "Selecting split value for variable 1:\n",
      "  Cut: < 5.60 Gini=0.493\n",
      "\n",
      "Selecting split value for variable 2:\n",
      "  Cut: < 4.80 Gini=0.331\n",
      "\n",
      "Selecting split value for variable 3:\n",
      "  Cut: < 1.80 Gini=0.315\n"
     ]
    }
   ],
   "source": [
    "split_iris = get_best_split(iris_data[30:], y_col_idx = -1, verbose = 1)\n",
    "# print('Split: [X%d < %.3f]' % ((split_iris['index']+1), split_iris['value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of building the tree has several steps:\n",
    "* First, we need to split on entire dataset to create root node.\n",
    "* Once we have a root node:\n",
    "    * Build terminal nodes.\n",
    "    * Recursive splitting.\n",
    "    * Building tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminal nodes** are the ones where the tree finds its max depht level. This can occur in three cases:\n",
    "* Max depth (hyperparameter) is reached.\n",
    "* Data of group to split is too small and specific, which could lead us to **overfitting**.\n",
    "* Data has only one type of group so we cannot keep splitting it.\n",
    "\n",
    "They also give us the output of our algorithm, choosing between the most common class in the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_terminal(group, y_col_idx, verbose = 0):\n",
    "    out = [row[y_col_idx] for row in group]\n",
    "    if verbose > 0: print(out)\n",
    "    return max(set(out), key=out.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_terminal(left, -1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive splitting\n",
    "\n",
    "Here we will process the loop for creating the tree as we described above, also checking for the stopping criteria when a node is terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(node, max_depth, min_group_size, depth, y_col_idx):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    \n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = make_terminal(left + right, y_col_idx)\n",
    "        return\n",
    "    \n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'] = make_terminal(left, y_col_idx)\n",
    "        node['right'] = make_terminal(right, y_col_idx)\n",
    "        return\n",
    "    \n",
    "    # process left child\n",
    "    if len(left) <= min_group_size:\n",
    "        node['left'] = make_terminal(left, y_col_idx)\n",
    "    else:\n",
    "        node['left'] = get_best_split(left, y_col_idx)\n",
    "        splitting(node['left'], max_depth, min_group_size, depth+1, y_col_idx)\n",
    "        \n",
    "    # process right child\n",
    "    if len(right) <= min_group_size:\n",
    "        node['right'] = make_terminal(right, y_col_idx)\n",
    "    else:\n",
    "        node['right'] = get_best_split(right, y_col_idx)\n",
    "        splitting(node['right'], max_depth, min_group_size, depth+1, y_col_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create the root node and call the prev function to create the entire tree recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_group_size, y_col_idx = -1):\n",
    "    root = get_best_split(train, y_col_idx)\n",
    "    splitting(root, max_depth, min_group_size, 1, y_col_idx)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'variable idx': 2,\n",
       " 'cut value': 3.0,\n",
       " 'left': {'variable idx': 0,\n",
       "  'cut value': 5.1,\n",
       "  'left': {'variable idx': 0,\n",
       "   'cut value': 4.9,\n",
       "   'left': {'variable idx': 0, 'cut value': 4.7, 'left': 0.0, 'right': 0.0},\n",
       "   'right': {'variable idx': 0, 'cut value': 4.9, 'left': 0.0, 'right': 0.0}},\n",
       "  'right': {'variable idx': 0, 'cut value': 5.1, 'left': 0.0, 'right': 0.0}},\n",
       " 'right': {'variable idx': 3,\n",
       "  'cut value': 1.8,\n",
       "  'left': {'variable idx': 2,\n",
       "   'cut value': 5.0,\n",
       "   'left': {'variable idx': 3, 'cut value': 1.7, 'left': 1.0, 'right': 2.0},\n",
       "   'right': {'variable idx': 3, 'cut value': 1.6, 'left': 2.0, 'right': 1.0}},\n",
       "  'right': {'variable idx': 2,\n",
       "   'cut value': 4.9,\n",
       "   'left': {'variable idx': 0, 'cut value': 6.0, 'left': 1.0, 'right': 2.0},\n",
       "   'right': {'variable idx': 0, 'cut value': 6.3, 'left': 2.0, 'right': 2.0}}}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth = 4\n",
    "min_size = 2\n",
    "\n",
    "tree = build_tree(iris_data, max_depth, min_size, y_col_idx = -1)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting values is a matter of following the previously created tree. This is also a recursive process that will stop when the checks find a terminal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, row):\n",
    "    if row[node['variable idx']] < node['cut value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 2.0 y_hat: 0.0\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(tree, iris_data[0])\n",
    "print(\"y: {} y_hat: {}\".format(row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict and evaluate for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decission_tree(train, test, max_depth, min_group_size, y_col_idx = -1):\n",
    "    tree = build_tree(train, max_depth, min_group_size, y_col_idx)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(tree, row)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9571428571428572"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(iris_data)\n",
    "training, test = iris_data[:80,:], iris_data[80:,:-1]\n",
    "test_y = iris_data[80:,-1]\n",
    "\n",
    "predictions = decission_tree(training, test, 2, 10)\n",
    "score = sum(np.equal(test_y, predictions)) / len(test_y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 2., 0., 2., 2., 0., 1., 1., 2., 0., 2., 0., 0., 2., 0., 1.,\n",
       "       2., 1., 1., 2., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 1., 2., 0., 0., 1., 1., 2., 0., 0., 0., 2., 0., 2., 0., 2.,\n",
       "       1., 0., 2., 0., 0., 2., 1., 0., 0., 1., 1., 1., 2., 0., 1., 2., 2.,\n",
       "       0., 1.])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 2., 0., 2., 2., 0., 1., 1., 2., 0., 2., 0., 0., 2., 0., 1.,\n",
       "       2., 1., 1., 2., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 2., 1., 1.,\n",
       "       0., 0., 1., 2., 0., 0., 1., 1., 2., 0., 0., 0., 2., 0., 2., 0., 2.,\n",
       "       1., 0., 2., 0., 0., 2., 1., 0., 0., 1., 1., 1., 2., 0., 1., 1., 2.,\n",
       "       0., 2.])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "- Be careful building too deep trees. Usually leads to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links:\n",
    "* None yet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
